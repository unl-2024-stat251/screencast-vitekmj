---
author: "Your Name"
date: "2023-05-04"
title: "Project: Screencast"
output: html
categories: project
---

# Project Description

For your final project (which will take the place of the final exam), you will be recording a screencast in the style of David Robinson's TidyTuesday screencasts.

You can find time-stamped, catalogued versions of some of David Robinson's screencasts [here](https://www.rscreencasts.com/). 

Requirements:

- Your screencast should be approximately 45 minutes long.
- Your screencast should show your analysis of a [TidyTuesday dataset from 2023](https://github.com/rfordatascience/tidytuesday)
- You should showcase at least 4 different techniques you've learned in Stat 251. Some examples include:

    - data cleaning (dplyr) verbs
    - reshaping data (tidyr)
    - working with dates and times (lubridate)
    - working with strings (stringr)
    - writing functions to modularize your code
    - visualizing your data effectively
    
Unlike David Robinson's screencasts, you will write a rough pseudocode "script" before you start recording. 
This will give you a rough outline of how to do the analysis and what things you intend to cover.

Your goal is to help a future Stat 251 student understand some of the topics covered in this class. 
So while David Robinson and others who record their screencasts live might not fully explain what he's doing, you should take the time to explain each technique you decide to use in a way that will help someone else understand.


There will be three deliverables for this project:

1. [Plan your dataset and topics](Dataset-Topics.qmd)
2. [Pseudocode script](pseudocode.qmd) uploaded to github repository
3. Screencast + github repository
    - Screencast uploaded to YouTube/YuJa
    - Approximate time index provided for each of the 4 techniques you're demonstrating ([examples](https://www.rscreencasts.com/))
    - Code uploaded to github repository

In lieu of the final exam, you will peer review two classmates' screencasts. 


```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
# tuesdata <- tidytuesdayR::tt_load('2024-03-26')

team_results <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-03-26/team-results.csv', show_col_types = F)
public_picks <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-03-26/public-picks.csv', show_col_types = F)
```

data exploration
```{r}
team_results <- select(team_results, TEAMID:GAMES) %>%
  select(-matches("RANK"))

modify <- mutate

team_results <- team_results %>%
  modify(
    PASE_per_game = PASE/GAMES,
    PAKE_per_game = PAKE/GAMES,
    PASEG = round(PASE_per_game, digits = 4),
    PAKEG = round(PAKE_per_game, digits = 4))

# Calculate correlation coefficient between PAKEG and PASEG
correlation <- cor(team_results$PAKEG, team_results$PASEG)

# Calculate R squared
r_squared <- correlation^2

team_results$R_squared <- r_squared
# Huh its all the exact same despite being generated from different sources


inner_join(team_results, public_picks, by = "TEAM")

```

```{r}
selected_team_results <- team_results %>%
  select(teams, pake, pase)

# Merge on teams in public_picks
merged_data <- public_picks %>%
  left_join(selected_team_results, by = "teams")

# View merged data
print(merged_data)
```